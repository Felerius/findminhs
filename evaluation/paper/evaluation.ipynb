{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import functools\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "import pandas as pd\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "INSTANCE_DIR = Path('../../../instances')\n",
    "TIMEOUT_SECS = 60**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for results_dir in Path('results').glob('*'):\n",
    "    combined_file = results_dir / 'combined.csv'\n",
    "    csv_files = list(results_dir.glob('**/*.csv'))\n",
    "    try:\n",
    "        csv_files.remove(combined_file)\n",
    "    except ValueError:\n",
    "        pass\n",
    "    with combined_file.open('w') as f:\n",
    "        for i, csv_file in enumerate(csv_files):\n",
    "            text = csv_file.read_text()\n",
    "            if i > 0:\n",
    "                text = text.split('\\n', maxsplit=1)[1]\n",
    "            f.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_instance_names = [\n",
    "    f.name for f in INSTANCE_DIR.glob('**/*.dat')\n",
    "    if f.relative_to(INSTANCE_DIR).parts[0] != 'outdated'\n",
    "]\n",
    "\n",
    "def load_combined_df(spec: dict[str, str]) -> pd.DataFrame:\n",
    "    dfs = (pd.read_csv(f'results/{dir_name}/combined.csv',\n",
    "                       index_col='file_name').add_suffix('_' + suffix)\n",
    "           for suffix, dir_name in spec.items())\n",
    "    combined_df = functools.reduce(lambda df1, df2: df1.join(df2), dfs)\n",
    "\n",
    "    # Make sure all datasets have one row per instance, even if the instances\n",
    "    # didn't finish running in any experiment (and thus has no row in any CSV)\n",
    "    return combined_df.reindex(all_instance_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How many vertices to check in the static packing costly discard rule?"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "edfe6049ab5675c7b9b2a1e09418e73100ac83442bc85973e9f2e271101b608a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('findminhs-q4Va14KR': pipenv)",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}